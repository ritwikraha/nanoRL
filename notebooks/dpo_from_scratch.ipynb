{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlchluAcV6QpzI1IbORS0U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwikraha/nanoRL/blob/experiments/notebooks/dpo_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwv6RkO4X7p4",
        "outputId": "aba457dd-0d64-4154-ba75-985ba1c36002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import json"
      ],
      "metadata": {
        "id": "1QsBw39YYZih"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO-aritra: replace this with reasonibg dataset\n",
        "sample_data = [\n",
        "    {\n",
        "        \"prompt\": \"What is the capital of France?\",\n",
        "        \"chosen\": \" The capital of France is Paris.\",\n",
        "        \"rejected\": \" The capital of France is Berlin.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Explain Newton's second law.\",\n",
        "        \"chosen\": \" Newton's second law states that force equals mass times acceleration.\",\n",
        "        \"rejected\": \" Newton's second law says gravity makes things fall.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "with open(\"dpo_data.json\", \"w\") as f:\n",
        "    json.dump(sample_data, f, indent=2)\n"
      ],
      "metadata": {
        "id": "bzH3e3J8YdvP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreferenceDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        prompt = sample[\"prompt\"]\n",
        "        chosen = sample[\"chosen\"]\n",
        "        rejected = sample[\"rejected\"]\n",
        "\n",
        "        chosen_enc = self.tokenizer(prompt + chosen, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_length)\n",
        "        rejected_enc = self.tokenizer(prompt + rejected, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_length)\n",
        "\n",
        "        return {\n",
        "            \"chosen_input_ids\": chosen_enc[\"input_ids\"].squeeze(0),\n",
        "            \"chosen_attention_mask\": chosen_enc[\"attention_mask\"].squeeze(0),\n",
        "            \"rejected_input_ids\": rejected_enc[\"input_ids\"].squeeze(0),\n",
        "            \"rejected_attention_mask\": rejected_enc[\"attention_mask\"].squeeze(0),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Y6y4j2v7ZR89"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_logps(model, input_ids, attention_mask):\n",
        "    \"\"\"\n",
        "    Computes the sum of log-probabilities of the predicted tokens,\n",
        "    ignoring padding tokens.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Causal language model (AutoModelForCausalLM)\n",
        "    - input_ids: token ids [batch_size, seq_len]\n",
        "    - attention_mask: binary mask for padded tokens\n",
        "\n",
        "    Returns:\n",
        "    - Sum of log-probs per sequence: [batch_size]\n",
        "    \"\"\"\n",
        "    # Get logits from model forward pass\n",
        "    with torch.no_grad() if not model.training else torch.enable_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits  # shape: [batch_size, seq_len, vocab_size]\n",
        "\n",
        "    # shift inputs and logits to align predictions with correct labels\n",
        "    shift_logits = logits[..., :-1, :].contiguous()  # [B, L-1, V]\n",
        "    shift_labels = input_ids[..., 1:].contiguous()   # [B, L-1]\n",
        "    shift_mask = attention_mask[..., 1:].contiguous()  # [B, L-1]\n",
        "\n",
        "    # log-probs over vocabulary\n",
        "    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
        "\n",
        "    # gather log-probs of the correct labels\n",
        "    selected_log_probs = torch.gather(log_probs, -1, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # mask out padding tokens\n",
        "    selected_log_probs = selected_log_probs * shift_mask\n",
        "\n",
        "    # sum all thelog-probs per example\n",
        "    return selected_log_probs.sum(dim=-1)  # [batch_size]"
      ],
      "metadata": {
        "id": "dbJtTBz7aHWH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dpo_loss(policy_chosen_logps, policy_rejected_logps,\n",
        "             ref_chosen_logps, ref_rejected_logps, beta=0.1):\n",
        "    \"\"\"\n",
        "    Computes DPO loss as per the paper:\n",
        "    L = -log σ[β(Δπ - Δπ_ref)]\n",
        "\n",
        "    Also returns the KL divergence between policy and reference distributions:\n",
        "    KL = 0.5 * [(π_c - π*_c)^2 + (π_r - π*_r)^2]\n",
        "\n",
        "    Parameters:\n",
        "    - policy_chosen_logps: log π(y_c | x)\n",
        "    - policy_rejected_logps: log π(y_r | x)\n",
        "    - ref_chosen_logps: log π*(y_c | x)\n",
        "    - ref_rejected_logps: log π*(y_r | x)\n",
        "    - beta: temperature scaling factor\n",
        "\n",
        "    Returns:\n",
        "    - loss: scalar DPO loss\n",
        "    - approx_kl: scalar KL divergence (average over batch)\n",
        "    \"\"\"\n",
        "    # compute preference gaps\n",
        "    pi_diff = policy_chosen_logps - policy_rejected_logps\n",
        "    ref_diff = ref_chosen_logps - ref_rejected_logps\n",
        "\n",
        "    # dpo loss--> encourage π_diff > ref_diff\n",
        "    logits = beta * (pi_diff - ref_diff)\n",
        "    loss = -torch.nn.functional.logsigmoid(logits).mean()\n",
        "\n",
        "    # approximate KL divergence between policy and reference (symmetric)\n",
        "    kl_chosen = (policy_chosen_logps - ref_chosen_logps) ** 2\n",
        "    kl_rejected = (policy_rejected_logps - ref_rejected_logps) ** 2\n",
        "    approx_kl = 0.5 * (kl_chosen + kl_rejected).mean()\n",
        "\n",
        "    return loss, approx_kl"
      ],
      "metadata": {
        "id": "7ypjAStNkwXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(batch, model, ref_model, beta=0.1):\n",
        "    # log-probs for policy model\n",
        "    chosen_logps = get_logps(model, batch[\"chosen_input_ids\"], batch[\"chosen_attention_mask\"])\n",
        "    rejected_logps = get_logps(model, batch[\"rejected_input_ids\"], batch[\"rejected_attention_mask\"])\n",
        "\n",
        "    # log-probs for reference model (frozen)\n",
        "    with torch.no_grad():\n",
        "        ref_chosen_logps = get_logps(ref_model, batch[\"chosen_input_ids\"], batch[\"chosen_attention_mask\"])\n",
        "        ref_rejected_logps = get_logps(ref_model, batch[\"rejected_input_ids\"], batch[\"rejected_attention_mask\"])\n",
        "\n",
        "    # compute the dpo loss and KL-Divergence\n",
        "    loss, kl = dpo_loss(\n",
        "        policy_chosen_logps=chosen_logps,\n",
        "        policy_rejected_logps=rejected_logps,\n",
        "        ref_chosen_logps=ref_chosen_logps,\n",
        "        ref_rejected_logps=ref_rejected_logps,\n",
        "        beta=beta\n",
        "    )\n",
        "\n",
        "    return loss, kl\n"
      ],
      "metadata": {
        "id": "xK7Jro1_aKii"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load models and  respective tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# gpt2 doesn't have pad_token by default\n",
        "# might not be needed for other models\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# TODO-ritwik: make this generic\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "ref_model.eval()\n",
        "for p in ref_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "ref_model.to(device)\n"
      ],
      "metadata": {
        "id": "JNaZHRHpaSMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "with open(\"dpo_data.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "dataset = PreferenceDataset(data, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(3):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    for batch in tqdm(dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        loss, kl = training_step(batch, model, ref_model, beta=0.1)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Loss: {loss.item():.4f} | KL: {kl.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "48GvqghIaf6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"dpo-model\")\n",
        "tokenizer.save_pretrained(\"dpo-model\")"
      ],
      "metadata": {
        "id": "STME9pk_b5F1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}