{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ritwikraha/nanoRL/blob/experiments/notebooks/dpo_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bwv6RkO4X7p4"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1QsBw39YYZih"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bzH3e3J8YdvP"
   },
   "outputs": [],
   "source": [
    "# TODO-aritra: replace this with reasonibg dataset\n",
    "sample_data = [\n",
    "    {\n",
    "        \"prompt\": \"What is the capital of France?\",\n",
    "        \"chosen\": \" The capital of France is Paris.\",\n",
    "        \"rejected\": \" The capital of France is Berlin.\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain Newton's second law.\",\n",
    "        \"chosen\": \" Newton's second law states that force equals mass times acceleration.\",\n",
    "        \"rejected\": \" Newton's second law says gravity makes things fall.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "with open(\"dpo_data.json\", \"w\") as f:\n",
    "    json.dump(sample_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Y6y4j2v7ZR89"
   },
   "outputs": [],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        prompt = sample[\"prompt\"]\n",
    "        chosen = sample[\"chosen\"]\n",
    "        rejected = sample[\"rejected\"]\n",
    "\n",
    "        chosen_enc = self.tokenizer(\n",
    "            prompt + chosen,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        rejected_enc = self.tokenizer(\n",
    "            prompt + rejected,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_enc[\"input_ids\"].squeeze(0),\n",
    "            \"chosen_attention_mask\": chosen_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"rejected_input_ids\": rejected_enc[\"input_ids\"].squeeze(0),\n",
    "            \"rejected_attention_mask\": rejected_enc[\"attention_mask\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "dbJtTBz7aHWH"
   },
   "outputs": [],
   "source": [
    "def get_logps(model, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes the sum of log-probabilities of the predicted tokens,\n",
    "    ignoring padding tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Causal language model (AutoModelForCausalLM)\n",
    "    - input_ids: token ids [batch_size, seq_len]\n",
    "    - attention_mask: binary mask for padded tokens\n",
    "\n",
    "    Returns:\n",
    "    - Sum of log-probs per sequence: [batch_size]\n",
    "    \"\"\"\n",
    "    # Get logits from model forward pass\n",
    "    with torch.no_grad() if not model.training else torch.enable_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "    # shift inputs and logits to align predictions with correct labels\n",
    "    shift_logits = logits[..., :-1, :].contiguous()  # [B, L-1, V]\n",
    "    shift_labels = input_ids[..., 1:].contiguous()  # [B, L-1]\n",
    "    shift_mask = attention_mask[..., 1:].contiguous()  # [B, L-1]\n",
    "\n",
    "    # log-probs over vocabulary\n",
    "    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "    # gather log-probs of the correct labels\n",
    "    selected_log_probs = torch.gather(\n",
    "        log_probs, -1, shift_labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    # mask out padding tokens\n",
    "    selected_log_probs = selected_log_probs * shift_mask\n",
    "\n",
    "    # sum all thelog-probs per example\n",
    "    return selected_log_probs.sum(dim=-1)  # [batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzIPFDsjk9pT"
   },
   "source": [
    "# Direct Preference Optimization (DPO) Loss\n",
    "\n",
    "**Paper**: [https://arxiv.org/abs/2305.18290](https://arxiv.org/abs/2305.18290)\n",
    "\n",
    "**Title**: \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\"\n",
    "\n",
    "##  Formula\n",
    "\n",
    "$L_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x,y_w,y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\beta \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right) \\right]$\n",
    "\n",
    "Where:\n",
    "- $\\pi_\\theta$ is the policy model being trained\n",
    "- $\\pi_{\\text{ref}}$ is the reference model (frozen)\n",
    "- $y_w$ is the chosen (winning) response\n",
    "- $y_l$ is the rejected (losing) response\n",
    "- $\\beta$ is the temperature parameter\n",
    "- $\\sigma$ is the sigmoid function\n",
    "\n",
    "## Alternative Formula (log space)\n",
    "\n",
    "$L_{\\text{DPO}} = -\\mathbb{E} \\left[ \\log \\sigma \\left( \\beta \\left( \\log \\pi_\\theta(y_w | x) - \\log \\pi_\\theta(y_l | x) - \\log \\pi_{\\text{ref}}(y_w | x) + \\log \\pi_{\\text{ref}}(y_l | x) \\right) \\right) \\right]$\n",
    "\n",
    "DPO directly optimizes the policy to prefer chosen responses over rejected ones, while staying close to the reference model through the implicit KL regularization term (controlled by β)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "7ypjAStNkwXS"
   },
   "outputs": [],
   "source": [
    "def dpo_loss(\n",
    "    policy_chosen_logps,\n",
    "    policy_rejected_logps,\n",
    "    ref_chosen_logps,\n",
    "    ref_rejected_logps,\n",
    "    beta=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    dpo loss formula:\n",
    "    L = -E[log σ(β(log π(y_w|x) - log π(y_l|x) - log π_ref(y_w|x) + log π_ref(y_l|x)))]\n",
    "\n",
    "    where y_w is chosen (preferred) and y_l is rejected\n",
    "    σ is sigmoid, β controls kl regularization strength\n",
    "    \"\"\"\n",
    "    # compute preference gaps\n",
    "    policy_logits = policy_chosen_logps - policy_rejected_logps\n",
    "    ref_logits = ref_chosen_logps - ref_rejected_logps\n",
    "\n",
    "    # dpo loss: encourage policy_logits > ref_logits\n",
    "    logits = beta * (policy_logits - ref_logits)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "\n",
    "    # kl divergence: kl(π || π_ref) = E[log π - log π_ref]\n",
    "    kl_chosen = policy_chosen_logps - ref_chosen_logps\n",
    "    kl_rejected = policy_rejected_logps - ref_rejected_logps\n",
    "    kl_div = 0.5 * (kl_chosen.mean() + kl_rejected.mean())\n",
    "\n",
    "    # additional metrics for monitoring training\n",
    "    with torch.no_grad():\n",
    "        # how often does policy prefer chosen over rejected?\n",
    "        rewards_accuracy = (policy_logits > 0).float().mean()\n",
    "\n",
    "        # average preference margins\n",
    "        rewards_margin = policy_logits.mean()\n",
    "        ref_rewards_margin = ref_logits.mean()\n",
    "\n",
    "        # implicit reward from dpo paper: r(x,y) = β * log(π(y|x) / π_ref(y|x))\n",
    "        implicit_rewards_chosen = beta * (policy_chosen_logps - ref_chosen_logps)\n",
    "        implicit_rewards_rejected = beta * (policy_rejected_logps - ref_rejected_logps)\n",
    "        implicit_rewards_margin = (\n",
    "            implicit_rewards_chosen - implicit_rewards_rejected\n",
    "        ).mean()\n",
    "\n",
    "    metrics = {\n",
    "        \"rewards_accuracy\": rewards_accuracy,\n",
    "        \"rewards_margin\": rewards_margin,\n",
    "        \"ref_rewards_margin\": ref_rewards_margin,\n",
    "        \"implicit_rewards_margin\": implicit_rewards_margin,\n",
    "        \"kl_div\": kl_div,\n",
    "    }\n",
    "\n",
    "    return loss, kl_div, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xK7Jro1_aKii"
   },
   "outputs": [],
   "source": [
    "def training_step(batch, model, ref_model, beta=0.1):\n",
    "    # log-probs for policy model\n",
    "    chosen_logps = get_logps(\n",
    "        model, batch[\"chosen_input_ids\"], batch[\"chosen_attention_mask\"]\n",
    "    )\n",
    "    rejected_logps = get_logps(\n",
    "        model, batch[\"rejected_input_ids\"], batch[\"rejected_attention_mask\"]\n",
    "    )\n",
    "\n",
    "    # log-probs for reference model (frozen)\n",
    "    with torch.no_grad():\n",
    "        ref_chosen_logps = get_logps(\n",
    "            ref_model, batch[\"chosen_input_ids\"], batch[\"chosen_attention_mask\"]\n",
    "        )\n",
    "        ref_rejected_logps = get_logps(\n",
    "            ref_model, batch[\"rejected_input_ids\"], batch[\"rejected_attention_mask\"]\n",
    "        )\n",
    "\n",
    "    # compute the dpo loss and metrics\n",
    "    loss, kl, metrics = dpo_loss(\n",
    "        policy_chosen_logps=chosen_logps,\n",
    "        policy_rejected_logps=rejected_logps,\n",
    "        ref_chosen_logps=ref_chosen_logps,\n",
    "        ref_rejected_logps=ref_rejected_logps,\n",
    "        beta=beta,\n",
    "    )\n",
    "\n",
    "    return loss, kl, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNaZHRHpaSMI",
    "outputId": "155f22c4-2ca4-4bb8-e924-e67221496a59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load models and respective tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# gpt2 doesn't have pad_token by default\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "ref_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48GvqghIaf6j",
    "outputId": "ad026372-5845-4284-ca33-d80590ec1d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:27<00:00, 87.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8778 | KL: -9.8716 | Acc: 0.500 | Policy Margin: -2.1212 | Ref Margin: 1.0552 | Implicit Reward: -0.3176\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6609 | KL: -8.4320 | Acc: 0.500 | Policy Margin: 1.7704 | Ref Margin: 1.0552 | Implicit Reward: 0.0715\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9314 | KL: -9.3805 | Acc: 0.500 | Policy Margin: -2.2133 | Ref Margin: 1.0552 | Implicit Reward: -0.3268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "with open(\"dpo_data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dataset = PreferenceDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        loss, kl, metrics = training_step(batch, model, ref_model, beta=0.1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # log all metrics:\n",
    "        # - Loss: How much the model needs to improve - should decrease\n",
    "\n",
    "        # - KL Divergence: How much the model has changed from the original should stay stable\n",
    "\n",
    "        # - Accuracy: Percentage of time model prefers chosen over rejected responses - should increase\n",
    "\n",
    "        # - Policy Margin: How much current model prefers good responses over bad ones - should increase\n",
    "\n",
    "        # - Reference Margin: How much original model prefers good responses over bad ones - should stay constant\n",
    "\n",
    "        # - Implicit Reward: Internal reward score the model learns for chosen vs rejected - should increase\n",
    "\n",
    "        print(\n",
    "            f\"Loss: {loss.item():.4f} | KL: {kl.item():.4f} | \"\n",
    "            f\"Acc: {metrics['rewards_accuracy'].item():.3f} | \"\n",
    "            f\"Policy Margin: {metrics['rewards_margin'].item():.4f} | \"\n",
    "            f\"Ref Margin: {metrics['ref_rewards_margin'].item():.4f} | \"\n",
    "            f\"Implicit Reward: {metrics['implicit_rewards_margin'].item():.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STME9pk_b5F1",
    "outputId": "0ab9d83b-d51f-4888-edde-ed1714f12002"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dpo-model/tokenizer_config.json',\n",
       " 'dpo-model/special_tokens_map.json',\n",
       " 'dpo-model/vocab.json',\n",
       " 'dpo-model/merges.txt',\n",
       " 'dpo-model/added_tokens.json',\n",
       " 'dpo-model/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"dpo-model\")\n",
    "tokenizer.save_pretrained(\"dpo-model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDfJZ8xUCd7WpFGdiEXx0m",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}