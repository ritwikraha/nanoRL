{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOmuef9xhAWVhGzQFtZuumO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwikraha/nanoRL/blob/experiments/notebooks/verbal_reasoning_synthetic_pair_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r51Bxhm0ZP9i"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "-GlAEFSO4DGZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "9YtURkBn5HRH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')"
      ],
      "metadata": {
        "id": "yvZPFW1-4Kao"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_preference_data(num_samples=2000):\n",
        "    \"\"\"\n",
        "    Generates a dataset of verbal reasoning prompts, then creates \"chosen\" (correct)\n",
        "    and \"rejected\" (incorrect) responses for each, with both answers and reasoning.\n",
        "\n",
        "    Args:\n",
        "        num_samples (int): The total number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        str: A CSV formatted string containing the generated data.\n",
        "             Returns an empty string if no data is generated.\n",
        "    \"\"\"\n",
        "    print(f\"INFO: Starting data generation for {num_samples} samples...\")\n",
        "\n",
        "    # Topics designed to create verbal reasoning questions\n",
        "    topics = [\n",
        "        \"a logical analogy problem (e.g., 'Leaf is to Tree as Page is to X')\",\n",
        "        \"a short scenario requiring a logical deduction to find the outcome\",\n",
        "        \"identifying the unstated, underlying assumption in a given argument\",\n",
        "        \"determining the most logical cause for a specifically described effect\",\n",
        "        \"finding the word that doesn't belong in a group based on a shared category\",\n",
        "        \"evaluating a simple argument to pinpoint its primary logical flaw\",\n",
        "        \"drawing a single, valid conclusion from a short passage of text\",\n",
        "        \"completing a logical sequence of related words or concepts\",\n",
        "        \"identifying the specific relationship between two words (like cause/effect)\",\n",
        "        \"solving a simple syllogism to determine if the conclusion is valid\"\n",
        "    ]\n",
        "\n",
        "    generated_data = []\n",
        "    while len(generated_data) < num_samples:\n",
        "        try:\n",
        "            topic = pd.Series(topics).sample(1).iloc[0]\n",
        "\n",
        "            # refined prompt to get ONLY the question text from the model\n",
        "            prompt_instruction = f\"Generate a single, clear question that is an example of the following verbal reasoning task: {topic}. Output nothing but the question itself, without any labels, formatting, or introductory text.\"\n",
        "\n",
        "            question_response = model.generate_content(prompt_instruction)\n",
        "            prompt = question_response.text.strip()\n",
        "\n",
        "            # prompt to generate the chosen/rejected pair with 'answer' and 'reasoning'\n",
        "            generation_prompt = f\"\"\"\n",
        "            For the verbal reasoning question: \"{prompt}\"\n",
        "\n",
        "            Please generate two distinct answers, each with an 'answer' and 'reasoning' field:\n",
        "            1.  A logically sound and well-explained correct answer.\n",
        "            2.  A plausible-sounding but logically flawed or incorrect answer.\n",
        "\n",
        "            Format your response as a single, clean JSON object with two keys: \"chosen\" and \"rejected\".\n",
        "            Each value (for \"chosen\" and \"rejected\") should itself be a JSON object with two keys: \"answer\" and \"reasoning\".\n",
        "\n",
        "            Example:\n",
        "            {{\n",
        "                \"chosen\": {{\n",
        "                    \"answer\": \"Correct answer text.\",\n",
        "                    \"reasoning\": \"Detailed explanation of why this answer is correct.\"\n",
        "                }},\n",
        "                \"rejected\": {{\n",
        "                    \"answer\": \"Incorrect answer text.\",\n",
        "                    \"reasoning\": \"Detailed explanation of why this answer is flawed or incorrect.\"\n",
        "                }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            response = model.generate_content(generation_prompt)\n",
        "            # The model sometimes wraps its output in ```json ... ```, this removes it.\n",
        "            cleaned_response = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "            response_json = json.loads(cleaned_response)\n",
        "            if \"chosen\" in response_json and \"rejected\" in response_json:\n",
        "                # Ensure the 'chosen' and 'rejected' values are dictionaries with 'answer' and 'reasoning'\n",
        "                if isinstance(response_json[\"chosen\"], dict) and \\\n",
        "                   \"answer\" in response_json[\"chosen\"] and \"reasoning\" in response_json[\"chosen\"] and \\\n",
        "                   isinstance(response_json[\"rejected\"], dict) and \\\n",
        "                   \"answer\" in response_json[\"rejected\"] and \"reasoning\" in response_json[\"rejected\"]:\n",
        "\n",
        "                    generated_data.append({\n",
        "                        \"prompt\": prompt,\n",
        "                        \"chosen\": response_json[\"chosen\"],\n",
        "                        \"rejected\": response_json[\"rejected\"],\n",
        "                    })\n",
        "                    print(f\"SUCCESS: Generated sample {len(generated_data)}/{num_samples}\")\n",
        "                else:\n",
        "                    print(\"WARNING: Skipping sample due to 'chosen' or 'rejected' not having expected 'answer'/'reasoning' format.\")\n",
        "            else:\n",
        "                print(\"WARNING: Skipping sample due to missing 'chosen' or 'rejected' key.\")\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"WARNING: Skipping sample due to a JSON decoding error: {e}. Raw response: {cleaned_response}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: An unexpected error occurred: {e}. Retrying in 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "\n",
        "    if not generated_data:\n",
        "        print(\"INFO: No data was generated.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Convert the list of dictionaries to a pandas DataFrame and then to a CSV string\n",
        "    df = pd.DataFrame(generated_data)\n",
        "    print(\"\\nINFO: Data generation complete. Converting to CSV format.\")\n",
        "    return df.to_csv(index=False)"
      ],
      "metadata": {
        "id": "A550WoKc7B5y"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_data_csv = generate_preference_data(num_samples=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hx3GZsXb7nBA",
        "outputId": "52229235-6ad5-4f85-cdbc-ed7d229c9e78"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Starting data generation for 2000 samples...\n",
            "SUCCESS: Generated sample 1/2000\n",
            "SUCCESS: Generated sample 2/2000\n",
            "SUCCESS: Generated sample 3/2000\n",
            "SUCCESS: Generated sample 4/2000\n",
            "SUCCESS: Generated sample 5/2000\n",
            "SUCCESS: Generated sample 6/2000\n",
            "SUCCESS: Generated sample 7/2000\n",
            "SUCCESS: Generated sample 8/2000\n",
            "SUCCESS: Generated sample 9/2000\n",
            "SUCCESS: Generated sample 10/2000\n",
            "SUCCESS: Generated sample 11/2000\n",
            "SUCCESS: Generated sample 12/2000\n",
            "SUCCESS: Generated sample 13/2000\n",
            "SUCCESS: Generated sample 14/2000\n",
            "SUCCESS: Generated sample 15/2000\n",
            "SUCCESS: Generated sample 16/2000\n",
            "SUCCESS: Generated sample 17/2000\n",
            "SUCCESS: Generated sample 18/2000\n",
            "SUCCESS: Generated sample 19/2000\n",
            "SUCCESS: Generated sample 20/2000\n",
            "SUCCESS: Generated sample 21/2000\n",
            "SUCCESS: Generated sample 22/2000\n",
            "SUCCESS: Generated sample 23/2000\n",
            "SUCCESS: Generated sample 24/2000\n",
            "SUCCESS: Generated sample 25/2000\n",
            "SUCCESS: Generated sample 26/2000\n",
            "SUCCESS: Generated sample 27/2000\n",
            "SUCCESS: Generated sample 28/2000\n",
            "SUCCESS: Generated sample 29/2000\n",
            "SUCCESS: Generated sample 30/2000\n",
            "SUCCESS: Generated sample 31/2000\n",
            "SUCCESS: Generated sample 32/2000\n",
            "SUCCESS: Generated sample 33/2000\n",
            "SUCCESS: Generated sample 34/2000\n",
            "SUCCESS: Generated sample 35/2000\n",
            "SUCCESS: Generated sample 36/2000\n",
            "SUCCESS: Generated sample 37/2000\n",
            "SUCCESS: Generated sample 38/2000\n",
            "SUCCESS: Generated sample 39/2000\n",
            "SUCCESS: Generated sample 40/2000\n",
            "SUCCESS: Generated sample 41/2000\n",
            "SUCCESS: Generated sample 42/2000\n",
            "SUCCESS: Generated sample 43/2000\n",
            "SUCCESS: Generated sample 44/2000\n",
            "SUCCESS: Generated sample 45/2000\n",
            "SUCCESS: Generated sample 46/2000\n",
            "SUCCESS: Generated sample 47/2000\n",
            "SUCCESS: Generated sample 48/2000\n",
            "SUCCESS: Generated sample 49/2000\n",
            "SUCCESS: Generated sample 50/2000\n",
            "SUCCESS: Generated sample 51/2000\n",
            "SUCCESS: Generated sample 52/2000\n",
            "SUCCESS: Generated sample 53/2000\n",
            "SUCCESS: Generated sample 54/2000\n",
            "SUCCESS: Generated sample 55/2000\n",
            "SUCCESS: Generated sample 56/2000\n",
            "SUCCESS: Generated sample 57/2000\n",
            "SUCCESS: Generated sample 58/2000\n",
            "SUCCESS: Generated sample 59/2000\n",
            "SUCCESS: Generated sample 60/2000\n",
            "SUCCESS: Generated sample 61/2000\n",
            "SUCCESS: Generated sample 62/2000\n",
            "SUCCESS: Generated sample 63/2000\n",
            "SUCCESS: Generated sample 64/2000\n",
            "SUCCESS: Generated sample 65/2000\n",
            "SUCCESS: Generated sample 66/2000\n",
            "SUCCESS: Generated sample 67/2000\n",
            "SUCCESS: Generated sample 68/2000\n",
            "SUCCESS: Generated sample 69/2000\n",
            "SUCCESS: Generated sample 70/2000\n",
            "SUCCESS: Generated sample 71/2000\n",
            "SUCCESS: Generated sample 72/2000\n",
            "SUCCESS: Generated sample 73/2000\n",
            "SUCCESS: Generated sample 74/2000\n",
            "SUCCESS: Generated sample 75/2000\n",
            "SUCCESS: Generated sample 76/2000\n",
            "SUCCESS: Generated sample 77/2000\n",
            "SUCCESS: Generated sample 78/2000\n",
            "SUCCESS: Generated sample 79/2000\n",
            "SUCCESS: Generated sample 80/2000\n",
            "SUCCESS: Generated sample 81/2000\n",
            "SUCCESS: Generated sample 82/2000\n",
            "SUCCESS: Generated sample 83/2000\n",
            "SUCCESS: Generated sample 84/2000\n",
            "SUCCESS: Generated sample 85/2000\n",
            "SUCCESS: Generated sample 86/2000\n",
            "SUCCESS: Generated sample 87/2000\n",
            "SUCCESS: Generated sample 88/2000\n",
            "SUCCESS: Generated sample 89/2000\n",
            "SUCCESS: Generated sample 90/2000\n",
            "SUCCESS: Generated sample 91/2000\n",
            "SUCCESS: Generated sample 92/2000\n",
            "SUCCESS: Generated sample 93/2000\n",
            "SUCCESS: Generated sample 94/2000\n",
            "SUCCESS: Generated sample 95/2000\n",
            "SUCCESS: Generated sample 96/2000\n",
            "SUCCESS: Generated sample 97/2000\n",
            "SUCCESS: Generated sample 98/2000\n",
            "SUCCESS: Generated sample 99/2000\n",
            "SUCCESS: Generated sample 100/2000\n",
            "SUCCESS: Generated sample 101/2000\n",
            "SUCCESS: Generated sample 102/2000\n",
            "SUCCESS: Generated sample 103/2000\n",
            "SUCCESS: Generated sample 104/2000\n",
            "SUCCESS: Generated sample 105/2000\n",
            "SUCCESS: Generated sample 106/2000\n",
            "SUCCESS: Generated sample 107/2000\n",
            "SUCCESS: Generated sample 108/2000\n",
            "SUCCESS: Generated sample 109/2000\n",
            "SUCCESS: Generated sample 110/2000\n",
            "SUCCESS: Generated sample 111/2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.90ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 657.66ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.66ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 633.02ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.65ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 633.26ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 633.01ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.54ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 810.30ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 634.38ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.93ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.76ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.58ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 708.32ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 734.05ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.75ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 633.09ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.56ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 885.19ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 632.95ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 759.59ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: An unexpected error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-39-4065309460.py\u001b[0m in \u001b[0;36mgenerate_preference_data\u001b[0;34m(num_samples)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mquestion_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_instruction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTooManyRequests\u001b[0m: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-40-1278095473.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated_data_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_preference_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-39-4065309460.py\u001b[0m in \u001b[0;36mgenerate_preference_data\u001b[0;34m(num_samples)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ERROR: An unexpected error occurred: {e}. Retrying in 5 seconds...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1e55b87d",
        "outputId": "a32b57b5-7a48-4000-acb7-59974fe12c18"
      },
      "source": [
        "# Create a temporary file with the CSV data\n",
        "file_path = 'verbal_reasoning_dataset_synthetic_gemini_flash_100.csv'\n",
        "with open(file_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(generated_data_csv)\n",
        "\n",
        "# Trigger the download\n",
        "files.download(file_path)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a592fa33-1da0-4f6a-b0e0-602901d4fb58\", \"verbal_reasoning_dataset_synthetic_gemini_flash_100.csv\", 7482)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Add upload to HuggingFace"
      ],
      "metadata": {
        "id": "Y_lb9MIDWoPi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}