{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ritwikraha/nanoRL/blob/experiments/notebooks/verbal_reasoning_synthetic_pair_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r51Bxhm0ZP9i"
   },
   "outputs": [],
   "source": [
    "!pip install -q google-genai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GlAEFSO4DGZ"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEizezZxoEME"
   },
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvZPFW1-4Kao"
   },
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=userdata.get(\"GEMINI_API_KEY\"))\n",
    "model = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A550WoKc7B5y"
   },
   "outputs": [],
   "source": [
    "def generate_preference_data(num_samples=2000):\n",
    "    topics = [\n",
    "        \"a logical analogy problem (e.g., 'Leaf is to Tree as Page is to X')\",\n",
    "        \"a short scenario requiring a logical deduction to find the outcome\",\n",
    "        \"identifying the unstated, underlying assumption in a given argument\",\n",
    "        \"determining the most logical cause for a specifically described effect\",\n",
    "        \"finding the word that doesn't belong in a group based on a shared category\",\n",
    "        \"evaluating a simple argument to pinpoint its primary logical flaw\",\n",
    "        \"drawing a single, valid conclusion from a short passage of text\",\n",
    "        \"completing a logical sequence of related words or concepts\",\n",
    "        \"identifying the specific relationship between two words (like cause/effect)\",\n",
    "        \"solving a simple syllogism to determine if the conclusion is valid\",\n",
    "    ]\n",
    "\n",
    "    generated_data = []\n",
    "    while len(generated_data) < num_samples:\n",
    "        try:\n",
    "            topic = random.choice(topics)\n",
    "\n",
    "            prompt_instruction = f\"Generate a single, clear question that is an example of the following verbal reasoning task: {topic}. Output nothing but the question itself, without any labels, formatting, or introductory text.\"\n",
    "            # prompt = model.generate_content(prompt_instruction)\n",
    "            prompt = client.models.generate_content(\n",
    "                model=model, contents=prompt_instruction\n",
    "            )\n",
    "            prompt = prompt.text.strip()\n",
    "\n",
    "            # prompt to generate the chosen/rejected pair with 'answer' and 'reasoning'\n",
    "            generation_prompt = f\"\"\"\\\n",
    "For the verbal reasoning question: \"{prompt}\"\n",
    "\n",
    "Please generate two distinct answers, each with an 'answer' and 'reasoning' field:\n",
    "1.  A logically sound and well-explained correct answer.\n",
    "2.  A plausible-sounding but logically flawed or incorrect answer.\n",
    "\n",
    "Format your response as a single, clean JSON object with two keys: \"chosen\" and \"rejected\".\n",
    "Each value (for \"chosen\" and \"rejected\") should itself be a JSON object with two keys: \"answer\" and \"reasoning\".\n",
    "\n",
    "Example:\n",
    "{{\n",
    "    \"chosen\": {{\n",
    "        \"answer\": \"Correct answer text.\",\n",
    "        \"reasoning\": \"Detailed explanation of why this answer is correct.\"\n",
    "    }},\n",
    "    \"rejected\": {{\n",
    "        \"answer\": \"Incorrect answer text.\",\n",
    "        \"reasoning\": \"Detailed explanation of why this answer is flawed or incorrect.\"\n",
    "    }}\n",
    "}}\\\n",
    "\"\"\"\n",
    "\n",
    "            # response = model.generate_content(generation_prompt)\n",
    "            response = client.models.generate_content(\n",
    "                model=model, contents=generation_prompt\n",
    "            )\n",
    "            cleaned_response = (\n",
    "                response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            )\n",
    "\n",
    "            response_json = json.loads(cleaned_response)\n",
    "            if \"chosen\" in response_json and \"rejected\" in response_json:\n",
    "                # Ensure the 'chosen' and 'rejected' values are dictionaries with 'answer' and 'reasoning'\n",
    "                if (\n",
    "                    isinstance(response_json[\"chosen\"], dict)\n",
    "                    and \"answer\" in response_json[\"chosen\"]\n",
    "                    and \"reasoning\" in response_json[\"chosen\"]\n",
    "                    and isinstance(response_json[\"rejected\"], dict)\n",
    "                    and \"answer\" in response_json[\"rejected\"]\n",
    "                    and \"reasoning\" in response_json[\"rejected\"]\n",
    "                ):\n",
    "                    generated_data.append(\n",
    "                        {\n",
    "                            \"prompt\": prompt,\n",
    "                            \"chosen\": response_json[\"chosen\"],\n",
    "                            \"rejected\": response_json[\"rejected\"],\n",
    "                        }\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"SUCCESS: Generated sample {len(generated_data)}/{num_samples}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"WARNING: Skipping sample due to 'chosen' or 'rejected' not having expected 'answer'/'reasoning' format.\"\n",
    "                    )\n",
    "            else:\n",
    "                print(\n",
    "                    \"WARNING: Skipping sample due to missing 'chosen' or 'rejected' key.\"\n",
    "                )\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\n",
    "                f\"WARNING: Skipping sample due to a JSON decoding error: {e}. Raw response: {cleaned_response}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: An unexpected error occurred: {e}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    if not generated_data:\n",
    "        print(\"INFO: No data was generated.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame and then to a CSV string\n",
    "    return {\n",
    "        key: [data[key] for data in generated_data]\n",
    "        for key in [\"prompt\", \"chosen\", \"rejected\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hx3GZsXb7nBA"
   },
   "outputs": [],
   "source": [
    "generated_dataset = generate_preference_data(num_samples=2000)\n",
    "hf_ds = Dataset.from_dict(generated_dataset, split=\"train\")\n",
    "\n",
    "hf_ds.push_to_hub(\"ritwikraha/reasoning\", token=userdata.get(\"HF_WRITE_TOKEN\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNiVow5oRWA67qCsjRIUF6E",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}